<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representation</title>
  <meta name="description" content="BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representation">
  <meta name="keywords" content="BEVCALIB">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title" content="BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representation">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representation">
  <meta property="og:image" content="https://cisl.ucr.edu/BEVCalib/static/images/bevcalib_overview.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="1939" />
  <meta property="og:image:height" content="772" />
  <meta property="og:url" content="https://cisl.ucr.edu/BEVCalib" />
  <meta property="og:description" content="BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representation" />
  <meta name="twitter:title" content="BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representation" />
  <meta name="twitter:description" content="BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representation" />
  <meta name="twitter:image" content="https://cisl.ucr.edu/BEVCalib/static/images/bevcalib_overview.png" />
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    window.MathJax = {
        tex: {
            tags: 'ams'
        }
    };
</script>
  
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><span style="font-size:2.4rem;">BEVCALIB: LiDAR-Camera Calibration <br> via Geometry-Guided Bird's-Eye View Representation</span></h1>
            <div class="is-size-4 publication-title">
              <span><a href="https://www.corl.org/"><strong>CoRL 2025</strong></a></span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://yewandou7.github.io/">Weiduo Yuan</a><sup>*,1</sup>,
              </span>
              <span class="author-block">
                <a href="">Jerry Li</a><sup>*,2</sup>,
              </span>
              <span class="author-block">
                <a href="">Justin Yue</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="">Divyank Shah</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://profiles.ucr.edu/app/home/profile/karydis">Konstantinos Karydis</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://hangqiu.github.io/">Hang Qiu</a><sup>2</sup>
            </div>
            
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup><font size="-0.4">*</sup>Equal contribution</font></span><br>
              <span class="author-block"><sup>1</sup>University of Southern California,</span>
              <span class="author-block"><sup>2</sup>University of California, Riverside</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2506.02587"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/UCR-CISL/BEVCalib/tree/main"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Model Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/cisl-hf/BEVCalib"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/hf_icon.svg" />
                    </span>
                    <span>Models</span>
                  </a>
                </span>
                <!-- BibTex Link. -->
                <span class="link-block">
                  <a href="#BibTeX"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-book-open"></i>
                    </span>
                    <span>BibTex</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified has-text-centered">
            <p>
              Accurate LiDAR-camera calibration is the foundation of accurate multimodal fusion environmental perception for autonomous driving and robotic systems.
              Traditional calibration methods require extensive data collection in controlled environments and cannot compensate for the transformation changes during 
              the vehicle/robot movement. In this paper, we propose the first model that uses bird's-eye view (BEV) features to perform LiDAR camera calibration from raw data,
              termed <strong>BEVCALIB</strong>. To achieve this, we extract camera BEV features and LiDAR BEV features separately and fuse them into a shared BEV feature space. To fully 
              utilize the geometry information from the BEV feature, we introduce a novel feature selector to choose the most important feature in the transformation decoder, 
              which reduces memory consumption and enables efficient training. Extensive evaluations in various datasets demonstrate that BEVCALIB establishes a new state-of-the-art.
              Under various noise conditions, BEVCALIB outperforms the <strong><em>best baseline in literature</em></strong> by an average of (47.08%, 82.32%) on KITTI dataset, 
              and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation, rotation). In the open-source domain, it improves the <strong><em>best</em></strong> reproducible baseline by one order of magnitude.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Overall Architecture</h2>
          <div class="content has-text-justified has-text-centered">
            <img src="static/images/bevcalib_overview.png" />
            <p>
              The overall pipeline of our model consists of BEV feature extraction, FPN BEV Encoder, and geometry-guided BEV decoder (GGBD). For BEV feature extraction, the 
              inputs of the camera and LiDAR are extracted into BEV features through different backbones separately, which are then fused into a shared BEV feature space. The FPN BEV encoder is used 
              to improve the multi-scale geometric information of the BEV representations. For geometry-guided BEV decoder utilizes a novel feature selector that efficiently 
              decodes calibration parameters from BEV features. \(\mathcal{L}_R\) , \(\mathcal{L}_T\) , and \(\mathcal{L}_{PC}\) are rotation loss, translation loss, and lidar reprojection loss, respectively.
              We use total loss \(\mathcal{L} = \lambda_R \mathcal{L}_R + \lambda_T \mathcal{L}_T + \lambda_{PC}\mathcal{L}_{PC}\) to achieve end-to-end optimization.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Geometry-Guided BEV decoder(GGBD)</h2>
          <div class="content has-text-justified has-text-centered">
            <img src="static/images/bevcalib_ggbd.png" />
            <p>
              The GGBD component contains a feature selector (left) and a refinement module (right). The feature selector calculates the positions of BEV features using Equation 
              \(P_\mathcal{B} = \text{Set}(\{\text{Proj}(p) | p\in P_C^W\}) \label{eq:proj}\).  
              The corresponding positional embeddings (PE) are added to keep the geometry information of the selected feature. After the decoder, the refinement module adds an 
              average-pooling operation to aggregate high-level information, following two separate heads to predict translation and rotation.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3">Experiments</h2>

                <h3 class="title is-4">Comparasion with Original Results from Literature</h3>

                <div class="content has-text-justified">
                  <img src="static/images/kitti.png" />
                  <img src="static/images/nuscenes.png" />
                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                    <tr>
                      <p>
                        Table 2 and Table 3 compares <strong>BEVCalib</strong> with original results reported in the literature. Since each of the existing model 
                        was trained and evaluated using different noise settings, we group them into different clusters and 
                        evaluate BEVCalib under the same noise settings for a fair comparison. On KITTI dataset,
                        BEVCalib has only a few centimeter translation error, outperforming the best baselines by an average
                        of 14.29% - 78.82%, and less than 0.1\(^\circ\) rotation error, outperforming the best baselines by an average of 71.43% - 95.70% under various noise conditions.
                        On NuScenes, BEVCalib has slightly
                        error but still outperforms the best baseline by 78.17% in translation, 68.29% in rotation. Notably, although BEVCalib is trained under the largest noise (\(\pm\)1.5m, \(\pm\)20\(^\circ\)), 
                        it shows extremely robustness when evaluated on smaller noise, overcoming the noise sensitivity that cripples previous methods such as LCCNet. 
                        In addition, BEVCalib demonstrates remarkable rotation prediction accuracy for all three angles (roll, pitch, yaw) with error below 0.2\(^\circ\), achieving a 
                        near-perfect result that outperforms any previous methods.
                      </p>
                    </tr>
                  </table>
                  
                </div>

                <br>

                <h3 class="title is-4">Evaluation Results with Open-Source Baselines</h3>
                <img src="static/images/opensource_result.png" />
                <div class="content has-text-justified">

                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                    <tr>
                      <p>
                        Table 4 compares <strong>BEVCalib</strong> with the selected baselines on KITTI, NuScenes, and CaslibDB. In our exhaustive effort searching for reproducible baselines, 
                        we find that the open-source space in this LiDAR-camera calibration domain is rather scarce (very few checkpoints) and underperforming despite the abundant literature. 
                        Hence, our open-source effort will significantly boost the performance of publicly availble calibration tools. Specifically, Table 4 shows that BEVCalib 
                        outperforms the best open-source baselines by (92.75%, 89.22%) on KITTI dataset and by (92.69%, 93.62%) on NuScenes dataset, in terms of (translation, rotation), respectively.
                        While BEVCalib approaches near-zero error on most, if not all, samples, CalibNet and Koide3 struggle with predicting the correct z-component while Regnet and CalibAnything 
                        struggle with all components on KITTI and NuScenes datasets. We apply a random noise between \([-1.5m, 1.5m]\) and \([-20^\circ, 20^\circ]\) to initial guess in this table.
                      </p>
                    </tr>
                  </table>
                </div>
                
                <br>
                <h3 class="title is-4">Qualitative results</h3>

                <div class="content has-text-justified">
                  <figure class="image has-text-centered">
                    <img src="static/images/overlay-comparison_high_resolution.png">
                  </figure>
                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                    <tr>
                      <p>
                        We visualize the predicted extrinsic calibration results by overlaying the LiDAR point cloud on the camera image. From top to bottom: ground-truth, 
                        BEVCalib, Koide3, CalibAnything, Regnet. Regnet and CalibAnything's overlays are misaligned due to the large error in rotation and translation, so the point cloud is 
                        not level with the ground. BEVCalib and Koide3 are closer to the ground-truth overlay, but there are objects where Koide3's overlay is slightly misaligned, 
                        e.g. the misaligned cars in the left column, the traffic sign in the middle column, and the pole and tree in the right column. In contrast, BEVCalib's 
                        overlays do not show these misalignments, indicating more accurate calibration results from BEVCalib across real environments.
                      </p>
                    </tr>
                  </table>
                </div>

                <br>
      </section>

      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-full-width">
              <h2 class="title is-3">Conclusion</h2>
              <div class="content has-text-justified has-text-centered">
                <p>
                  In this paper, we introduce <strong>BEVCalib</strong>, the first LiDAR-camera extrinsic calibration model using BEV features. Geometry-guided BEV decoder can effectively and 
                  efficiently capture scene geometry, enhancing calibration accuracy.  Results on KITTI, NuScenes, and our own indoor dataset with dynamic extrinsics illustrate 
                  that our approach establishes a new state of the art in learning-based calibration methods. Under various noise conditions, BEVCalib outperforms the 
                  best baseline in literature by an average of (47.08%, 82.32%) on KITTI dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation, rotation) respectively. Also, 
                  BEVCalib improves the best reproducible baseline by one order of magnitude, making an important contribution to the scarce open-source space in LiDAR-camera calibration.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>


      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>@inproceedings{bevcalib,
      title={BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representations}, 
      author={Weiduo Yuan and Jerry Li and Justin Yue and Divyank Shah and Konstantinos Karydis and Hang Qiu},
      booktitle={9th Annual Conference on Robot Learning},
      year={2025},
}
          </code></pre>
        </div>
      </section>


      <footer class="footer">
        <div class="container">
          <!-- <div class="content has-text-centered">
            <a class="icon-link" href="https://arxiv.org/pdf/2210.05714.pdf">
              <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="" class="external-link" disabled>
              <i class="fab fa-github"></i>
            </a>
          </div> -->
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.</p>

              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>
